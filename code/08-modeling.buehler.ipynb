{"cells":[{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":934,"status":"ok","timestamp":1682643917121,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"mDMQ0yqmBXoB"},"outputs":[],"source":["from google.colab import drive\n","import json\n","import os\n","import pandas as pd\n","import numpy as np\n","import random\n","import tensorflow as tf\n","import torch\n","from keras.models import Sequential, Model\n","from keras.layers import Conv2D, Conv3D, BatchNormalization, Activation, Dense, Flatten, Dropout, Input, concatenate, GlobalAveragePooling2D, GlobalAveragePooling3D, TimeDistributed, LSTM\n","from keras.losses import mean_squared_error, huber, log_cosh\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","from keras.regularizers import l1_l2, l2\n","from keras.utils import np_utils\n","from keras import backend as K\n","from sklearn.preprocessing import StandardScaler, LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import GridSearchCV, StratifiedKFold\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, precision_recall_curve, auc, accuracy_score, matthews_corrcoef, f1_score, confusion_matrix\n","from scipy import interp\n","import datetime\n","import h5py\n","from tensorflow.python.keras.saving import hdf5_format\n","import seaborn as sns\n","import pickle"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21639,"status":"ok","timestamp":1682640965905,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"uZAOs-tPs1ZK","outputId":"6a276c7b-9ef8-47d5-a232-219cdead843c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Set root path\n","root_path = \"drive/MyDrive/NNextPitch\"\n","\n","# Open data\n","with open(os.path.join(root_path, \"data/modeling-data/modeling-data.buehler.json\")) as f:\n","    data = json.load(f)"]},{"cell_type":"markdown","metadata":{"id":"wYOR7vKBsOdx"},"source":["# Modeling Preparation"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682640965905,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"W49wU7XFg0Dt"},"outputs":[],"source":["def extract_xy(data_dict: dict, outcome_key: str):\n","\n","  x = []\n","  y = []\n","\n","  for play_id in data_dict.keys():\n","\n","    # Skip NAs\n","    if type(data_dict[play_id][\"outcome2\"]) != str:\n","      print(f\"{play_id} has NA outcome. Skipping sample.\\n\")\n","\n","    else:\n","      # Get play-specific pose data\n","      pose_dict = data_dict[play_id]['pose_data']\n","\n","      # Re-organize play-specific data into shape (10, 15, 2)\n","      # Shape: \n","      # [[x0, y0], [x1, y1], ..., [x15, y15],\n","      #  [x0, y0], [x1, y1], ..., [x15, y15],\n","      #  ... 10dims, 1 per keypoint ...\n","      #  [x0, y0], [x1, y1], ..., [x15, y15]]\n","      play_output = []\n","      for keypoint in pose_dict.keys():\n","        xy_dict = pose_dict[keypoint]\n","        play_output.append(np.array([np.array(xy) for xy in zip(xy_dict['x'], xy_dict['y'])]))\n","      play_output = np.array(play_output)\n","\n","      # Add re-organized play-specific data to the full output set\n","      x.append(play_output)\n","\n","      # Extract y\n","      y.append(data_dict[play_id][outcome_key])\n","\n","  # Encode y\n","  outcomes = list(np.unique(y))\n","  outcome_map = {pitch_type: i for i, pitch_type in enumerate(outcomes)}\n","  y = pd.Series(y).map(outcome_map)\n","  print(f\"Outcome encoding:\\n{outcome_map}\")\n","\n","  # Return x, y\n","  return np.array(x), np.array(y)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1682640965906,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"qsfGFMju0FB8","outputId":"8b155d65-e943-4ddc-acd2-ea61599152f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Outcome encoding:\n","{'FB': 0, 'FBwM': 1, 'OFF': 2}\n"]}],"source":["# Extract features and outcomes from original data source\n","x, y = extract_xy(data, outcome_key=\"outcome1\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1682640965906,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"9acM6F3M0g25","outputId":"66ea6c6a-ee5c-4791-d673-fbef126cecd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["(896, 10, 15, 2)\n","(896,)\n"]}],"source":["# Check shapes of x and y\n","print(x.shape)\n","print(y.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682640965906,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"WN9uq4v5jO6V","outputId":"7260e43f-bfdc-45bc-de64-aa51eaac00e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["(896, 15, 10, 2)\n"]}],"source":["# Reshape to make time the third dimension\n","x = np.transpose(x, (0, 2, 1, 3))\n","print(x.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682640965906,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"0YJuBcLk17eB"},"outputs":[],"source":["# 80/20 split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.20, random_state=0)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682640965907,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"CGE5L27U2ZQf","outputId":"e4387cba-df5b-4473-fe91-cf3eac53031b"},"outputs":[{"output_type":"stream","name":"stdout","text":["(716, 15, 10, 2)\n","(180, 15, 10, 2)\n"]}],"source":["# Check the shape of each set\n","print(x_train.shape)\n","print(x_test.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1682640965907,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"vq-ZUKNE3Tn1","outputId":"5910bd06-904e-4bd5-cd62-bd8e9930f0cc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      y  train  test\n","0    FB    297    75\n","1   OFF    249    63\n","2  FBwM    170    42"],"text/html":["\n","  <div id=\"df-29b7167b-c682-463c-863c-92fbd371ddd1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>y</th>\n","      <th>train</th>\n","      <th>test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FB</td>\n","      <td>297</td>\n","      <td>75</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>OFF</td>\n","      <td>249</td>\n","      <td>63</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>FBwM</td>\n","      <td>170</td>\n","      <td>42</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29b7167b-c682-463c-863c-92fbd371ddd1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-29b7167b-c682-463c-863c-92fbd371ddd1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-29b7167b-c682-463c-863c-92fbd371ddd1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}],"source":["# Check y balance\n","pd.DataFrame({\n","    \"y\": [\"FB\", \"OFF\", \"FBwM\"],\n","    \"train\": list(pd.DataFrame(y_train).value_counts()), \n","    \"test\": list(pd.DataFrame(y_test).value_counts())})"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682640965908,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"ZkG9pkHCnTLB","outputId":"f4a78f17-34ee-41b4-bfc0-b92fad8d795b"},"outputs":[{"output_type":"stream","name":"stdout","text":["(716, 3)\n"]}],"source":["# Convert y_train to dummy variables\n","y_train_dummy = np_utils.to_categorical(y_train)\n","\n","# Check the shape\n","print(y_train_dummy.shape)"]},{"cell_type":"markdown","metadata":{"id":"tDpQdDtvsByt"},"source":["# Functions"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682640965908,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"b83BDxgjsB1S"},"outputs":[],"source":["def split_x_by_keypoint(x):\n","  '''\n","  Convert shape of (n, 10, 15, 2, 1) to 10X (n, 15, 2, 1)\n","  '''\n","\n","  x1, x2, x3, x4, x5, x6, x7, x8, x9, x10 = [], [], [], [], [], [], [], [], [], []\n","\n","  for observation in x:\n","    x1.append(observation[0])\n","    x2.append(observation[1])\n","    x3.append(observation[2])\n","    x4.append(observation[3])\n","    x5.append(observation[4])\n","    x6.append(observation[5])\n","    x7.append(observation[6])\n","    x8.append(observation[7])\n","    x9.append(observation[8])\n","    x10.append(observation[9])\n","\n","  return [\n","      np.array(x1), np.array(x2), np.array(x3), np.array(x4), np.array(x5), \n","      np.array(x6), np.array(x7), np.array(x8), np.array(x9), np.array(x10)]\n","  \n","\n","def class_report(y_true, y_pred, target_names_dict, y_score=None, average='micro'):\n","  '''\n","  Code adapted from https://stackoverflow.com/questions/39685740/calculate-sklearn-roc-auc-score-for-multi-class\n","  '''\n","\n","  if y_true.shape != y_pred.shape:\n","      print(\"Error! y_true %s is not the same shape as y_pred %s\" % (\n","            y_true.shape,\n","            y_pred.shape)\n","      )\n","      return\n","\n","  lb = LabelBinarizer()\n","\n","  if len(y_true.shape) == 1:\n","      lb.fit(y_true)\n","\n","  # Value counts of predictions\n","  labels, cnt = np.unique(y_pred, return_counts=True)\n","  named_labels = [target_names_dict[lab] for lab in labels]\n","  n_classes = len(labels)\n","  pred_cnt = pd.Series(cnt, index=named_labels)\n","\n","  metrics_summary = precision_recall_fscore_support(\n","      y_true=y_true,\n","      y_pred=y_pred,\n","      labels=labels)\n","  \n","  metrics_summary = [np.round(arr, 3) for arr in metrics_summary]\n","\n","  avg = list(precision_recall_fscore_support(\n","      y_true=y_true, \n","      y_pred=y_pred,\n","      average='weighted'))\n","  \n","  avg = [round(av, 3) for av in avg if av is not None] + [None]\n","\n","  metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n","  class_report_df = pd.DataFrame(\n","      list(metrics_summary),\n","      index=metrics_sum_index,\n","      columns=named_labels)\n","\n","  support = class_report_df.loc['support']\n","  total = support.sum()\n","  class_report_df['weighted avg'] = avg[:-1] + [total]\n","\n","  class_report_df = class_report_df.T\n","  class_report_df['pred'] = pred_cnt\n","  class_report_df['pred'].iloc[-1] = total\n","\n","  if not (y_score is None):\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    precision = dict()\n","    recall = dict()\n","    auprc = dict()\n","    for label_it, label in enumerate(labels):\n","      # ROC-AUC\n","      fpr[label], tpr[label], _ = roc_curve(\n","          (y_true == label).astype(int), \n","          y_score[:, label_it])\n","      roc_auc[label] = auc(fpr[label], tpr[label])\n","\n","      # AUPRC\n","      precision[label], recall[label], _ = precision_recall_curve(\n","          (y_true == label).astype(int), \n","          y_score[:, label_it])\n","      auprc[label] = auc(recall[label], precision[label])\n","\n","    if average == 'micro':\n","      # ROC-AUC\n","      fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n","          lb.transform(y_true).ravel(), \n","          y_score.ravel())\n","      roc_auc[\"weighted avg\"] = auc(fpr[\"avg / total\"], tpr[\"avg / total\"])\n","\n","      # AUPRC\n","      precision[\"avg / total\"], recall[\"avg / total\"], _ = precision_recall_curve(\n","          lb.transform(y_true).ravel(), \n","          y_score.ravel())\n","      auprc[\"weighted avg\"] = auc(recall[\"avg / total\"], precision[\"avg / total\"])\n","    \n","    else:\n","      print(\"No macro support yet. Only micro.\")\n","    \n","    target_names_dict[\"weighted avg\"] = \"weighted avg\"\n","    roc_auc = {target_names_dict[key]: round(value, 3) for key, value in roc_auc.items()}\n","    auprc = {target_names_dict[key]: round(value, 3) for key, value in auprc.items()}\n","    class_report_df['auc'] = roc_auc\n","    class_report_df['auprc'] = auprc\n","  \n","  class_report_df = class_report_df[[\"precision\", \"recall\", \"f1-score\", \"auc\", \"auprc\", \"support\", \"pred\"]]\n","  class_report_df['support'] = class_report_df['support'].astype('int')\n","  class_report_df['pred'] = class_report_df['pred'].astype('int')\n","\n","  return class_report_df\n","    \n","\n","def evaluate_predictions(y_true, y_test_score, y_test_pred, target_names_dict):\n","\n","  # Extract metrics\n","  test_accuracy = round(accuracy_score(y_true, y_test_pred), 3)\n","  test_mcc = round(matthews_corrcoef(y_true, y_test_pred), 3)\n","  test_f1 = round(f1_score(y_true, y_test_pred, average='weighted'), 3)\n","  classification_report = class_report(y_true=y_true, \n","    y_pred=y_test_pred, \n","    y_score=y_test_score, \n","    target_names_dict=target_names_dict)\n","  confusion_mat = confusion_matrix(y_true, y_test_pred)\n","\n","  # Print metrics\n","  print(f\"Accuracy: {test_accuracy}\")\n","  print(f\"MCC: {test_mcc}\")\n","  print(f\"F1 (Weighted): {test_f1}\")\n","  print(f\"Classification Report:\")\n","  print(classification_report)\n","  print(\"\")\n","\n","  return test_accuracy, test_mcc, test_f1, classification_report, confusion_mat\n"]},{"cell_type":"markdown","metadata":{"id":"lP_Q3ZbfsHtY"},"source":["# Models"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682641180212,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"dDskJeA9Mjqu"},"outputs":[],"source":["def CNN_tc_collapse(x_shape, dropout, l1, l2, learning_rate, n_targets):\n","  \n","  # Model architecture\n","  model = Sequential()\n","  model.add(Conv2D(8, (3,1), input_shape=x_shape))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv2D(16, (3,1)))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv2D(32, (3,1)))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv2D(64, (3,1)))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Flatten())\n","  model.add(Dense(256, activation='relu'))\n","  model.add(Dropout(dropout))\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dropout(dropout))\n","  model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(n_targets, activation=\"softmax\"))\n","\n","  # Compile model\n","  model.compile(\n","      loss='categorical_crossentropy', \n","      optimizer=tf.keras.optimizers.Adam(learning_rate),\n","      metrics=['accuracy'])\n","  \n","  return model\n","\n","\n","def CNN(x_shape, kernel_size, dropout, l1, l2, learning_rate, n_targets):\n","  \n","  # Model architecture\n","  model = Sequential()\n","  model.add(Conv3D(8, kernel_size, input_shape=x_shape))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv3D(16, kernel_size))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv3D(32, kernel_size))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv3D(64, kernel_size))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Flatten())\n","  model.add(Dense(256, activation='relu'))\n","  model.add(Dropout(dropout))\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dropout(dropout))\n","  model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(n_targets, activation=\"softmax\"))\n","\n","  # Compile model\n","  model.compile(\n","      loss='categorical_crossentropy', \n","      optimizer=tf.keras.optimizers.Adam(learning_rate),\n","      metrics=['accuracy'])\n","  \n","  return model\n","\n","\n","def CNN_LSTM(x_shape, kernel_size, dropout, l1, l2, learning_rate, n_targets):\n","  \n","  # Model architecture\n","  ## CNN\n","  model = Sequential()\n","  model.add(Conv3D(8, kernel_size, input_shape=x_shape))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv3D(16, kernel_size))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv3D(32, kernel_size))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Conv3D(64, kernel_size))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  ## LSTM\n","  model.add(TimeDistributed(Flatten()))\n","  model.add(LSTM(256, return_sequences=False))\n","  ## Dense Layers\n","  model.add(Dense(256, activation='relu'))\n","  model.add(Dropout(dropout))\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dropout(dropout))\n","  model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n","  model.add(Dense(n_targets, activation=\"softmax\"))\n","\n","  # Compile model\n","  model.compile(\n","    loss='categorical_crossentropy', \n","    optimizer=tf.keras.optimizers.Adam(learning_rate),\n","    metrics=['accuracy'])\n","  \n","  return model\n","\n","\n","def CNN_branched(x_shape, dropout, l1, l2, learning_rate, n_targets):\n","\n","  # Architecture of each branch\n","  def branch(input):\n","    conv1 = Conv2D(8, (3,1), activation='relu', padding='same')(input)\n","    batchnorm1 = BatchNormalization()(conv1)\n","    conv2 = Conv2D(16, (3,1), activation='relu', padding='same')(batchnorm1)\n","    batchnorm2 = BatchNormalization()(conv2)\n","    conv3 = Conv2D(32, (3,1), activation='relu', padding='same')(batchnorm2)\n","    batchnorm3 = BatchNormalization()(conv3)\n","    conv4 = Conv2D(64, (3,1), activation='relu', padding='same')(batchnorm3)\n","    batchnorm4 = BatchNormalization()(conv4)\n","    gap = GlobalAveragePooling2D()(batchnorm4)\n","    flattened_layer = Flatten()(gap)\n","    return flattened_layer\n","\n","  # Define the input layers for each input set\n","  input1 = Input(shape=x_shape)\n","  input2 = Input(shape=x_shape)\n","  input3 = Input(shape=x_shape)\n","  input4 = Input(shape=x_shape)\n","  input5 = Input(shape=x_shape)\n","  input6 = Input(shape=x_shape)\n","  input7 = Input(shape=x_shape)\n","  input8 = Input(shape=x_shape)\n","  input9 = Input(shape=x_shape)\n","  input10 = Input(shape=x_shape)\n","\n","  # Define the CNN branches for each input set\n","  cnn1_branch = branch(input1)\n","  cnn2_branch = branch(input2)\n","  cnn3_branch = branch(input3)\n","  cnn4_branch = branch(input4)\n","  cnn5_branch = branch(input5)\n","  cnn6_branch = branch(input6)\n","  cnn7_branch = branch(input7)\n","  cnn8_branch = branch(input8)\n","  cnn9_branch = branch(input9)\n","  cnn10_branch = branch(input10)\n","\n","  # Concatenate the output from each CNN branch\n","  concatenated_fc_input = concatenate([\n","      cnn1_branch, cnn2_branch, cnn3_branch, cnn4_branch, cnn5_branch,\n","      cnn6_branch, cnn7_branch, cnn8_branch, cnn9_branch, cnn10_branch])\n","\n","  # Define the fully connected layers to process the concatenated features\n","  fc1 = Dense(256, activation='relu')(concatenated_fc_input)\n","  dropout1 = Dropout(dropout)(fc1)\n","  fc2 = Dense(128, activation='relu')(dropout1)\n","  dropout2 = Dropout(dropout)(fc2)\n","  fc3 = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2))(dropout2)\n","  fc4 = Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2))(fc3)\n","  fc5 = Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2))(fc4)\n","  output = Dense(n_targets, activation='softmax')(fc5)  # Example output layer with 10 classes\n","\n","  # Create the model by specifying the inputs and outputs\n","  model = Model(\n","      inputs=[input1, input2, input3, input4, input5, input6, input7, input8, input9, input10], \n","      outputs=output)\n","\n","  # Compile model\n","  model.compile(\n","      loss='categorical_crossentropy', \n","      optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","      metrics=['accuracy'])\n","  \n","  return model\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682642059859,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"},"user_tz":360},"id":"J8xmdpuc6cwi"},"outputs":[],"source":["# Training parameters\n","# Epochs and batch size\n","epochs = 200\n","batch_size = 16\n","\n","# Model parameters\n","l1 = 1e-2\n","l2 = 1e-4\n","dropout = 0.3\n","\n","# Calculate class weights for loss\n","classWeights = compute_class_weight(\n","    class_weight='balanced', classes=np.unique(y_train), y=y_train)\n","classWeights = {0: classWeights[0], 1: classWeights[1], 2: classWeights[2]}\n","\n","# Early stopping call back to be called during training\n","early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=0, mode='min')\n","reduce_lr_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.005, patience=10, verbose=0, mode='min')\n","\n","# Target names for evaluation\n","target_names_dict={0: 'FB', 1: 'FBwM', 2: 'OFF'}\n"]},{"cell_type":"markdown","metadata":{"id":"i4Pwm11ztyc7"},"source":["# 5-Fold CV"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oj9DB-bkF05p","executionInfo":{"status":"ok","timestamp":1682643665215,"user_tz":360,"elapsed":1601381,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"}},"outputId":"ade5d6a1-d86c-4398-a0d1-6b76eb2e5237"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1\n","CNN (T/C Collapse)\n","6/6 [==============================] - 0s 2ms/step\n","Accuracy: 0.628\n","MCC: 0.427\n","F1 (Weighted): 0.629\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.692   0.720     0.706  0.817  0.733       75    78\n","FBwM              0.409   0.429     0.419  0.705  0.416       42    44\n","OFF               0.707   0.651     0.678  0.848  0.764       63    58\n","weighted avg      0.631   0.628     0.629  0.799  0.674      180   180\n","\n","CNN\n","6/6 [==============================] - 0s 3ms/step\n","Accuracy: 0.583\n","MCC: 0.351\n","F1 (Weighted): 0.576\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.619   0.693     0.654  0.768  0.728       75    84\n","FBwM              0.424   0.333     0.373  0.670  0.429       42    33\n","OFF               0.619   0.619     0.619  0.780  0.640       63    63\n","weighted avg      0.574   0.583     0.576  0.756  0.618      180   180\n","\n","CNN-LSTM\n","6/6 [==============================] - 0s 3ms/step\n","Accuracy: 0.572\n","MCC: 0.346\n","F1 (Weighted): 0.576\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.638   0.680     0.658  0.804  0.695       75    80\n","FBwM              0.347   0.405     0.374  0.607  0.298       42    49\n","OFF               0.686   0.556     0.614  0.774  0.653       63    51\n","weighted avg      0.587   0.572     0.576  0.748  0.601      180   180\n","\n","Branched-CNN\n","6/6 [==============================] - 1s 8ms/step\n","Accuracy: 0.489\n","MCC: 0.182\n","F1 (Weighted): 0.434\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.495   0.640     0.558  0.641  0.590       75    97\n","FBwM              0.333   0.024     0.044  0.629  0.390       42     3\n","OFF               0.488   0.619     0.545  0.719  0.624       63    80\n","weighted avg      0.455   0.489     0.434  0.660  0.509      180   180\n","\n","Fold 2\n","CNN (T/C Collapse)\n","6/6 [==============================] - 0s 2ms/step\n","Accuracy: 0.617\n","MCC: 0.407\n","F1 (Weighted): 0.615\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.658   0.693     0.675  0.810  0.740       75    79\n","FBwM              0.425   0.405     0.415  0.680  0.430       42    40\n","OFF               0.689   0.667     0.677  0.830  0.693       63    61\n","weighted avg      0.614   0.617     0.615  0.791  0.655      180   180\n","\n","CNN\n","6/6 [==============================] - 0s 2ms/step\n","Accuracy: 0.594\n","MCC: 0.368\n","F1 (Weighted): 0.584\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.646   0.680     0.662  0.781  0.670       75    79\n","FBwM              0.464   0.310     0.371  0.662  0.421       42    28\n","OFF               0.589   0.683     0.632  0.775  0.633       63    73\n","weighted avg      0.583   0.594     0.584  0.763  0.610      180   180\n","\n","CNN-LSTM\n","6/6 [==============================] - 0s 3ms/step\n","Accuracy: 0.611\n","MCC: 0.416\n","F1 (Weighted): 0.623\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.699   0.680     0.689  0.811  0.727       75    73\n","FBwM              0.362   0.500     0.420  0.667  0.390       42    58\n","OFF               0.776   0.603     0.679  0.842  0.763       63    49\n","weighted avg      0.647   0.611     0.623  0.787  0.663      180   180\n","\n","Branched-CNN\n","6/6 [==============================] - 1s 6ms/step\n","Accuracy: 0.539\n","MCC: 0.301\n","F1 (Weighted): 0.542\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.651   0.720     0.684  0.777  0.697       75    83\n","FBwM              0.291   0.381     0.330  0.617  0.410       42    55\n","OFF               0.643   0.429     0.514  0.737  0.599       63    42\n","weighted avg      0.564   0.539     0.542  0.723  0.579      180   180\n","\n","Fold 3\n","CNN (T/C Collapse)\n","6/6 [==============================] - 0s 2ms/step\n","Accuracy: 0.639\n","MCC: 0.469\n","F1 (Weighted): 0.655\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.754   0.693     0.722  0.854  0.787       75    69\n","FBwM              0.379   0.595     0.463  0.701  0.406       42    66\n","OFF               0.844   0.603     0.704  0.860  0.797       63    45\n","weighted avg      0.698   0.639     0.655  0.811  0.705      180   180\n","\n","CNN\n","6/6 [==============================] - 0s 2ms/step\n","Accuracy: 0.611\n","MCC: 0.409\n","F1 (Weighted): 0.619\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.750   0.720     0.735  0.841  0.777       75    72\n","FBwM              0.333   0.405     0.366  0.654  0.350       42    51\n","OFF               0.684   0.619     0.650  0.834  0.702       63    57\n","weighted avg      0.630   0.611     0.619  0.794  0.663      180   180\n","\n","CNN-LSTM\n","6/6 [==============================] - 0s 3ms/step\n","Accuracy: 0.533\n","MCC: 0.306\n","F1 (Weighted): 0.545\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.632   0.640     0.636  0.777  0.678       75    76\n","FBwM              0.303   0.476     0.370  0.630  0.368       42    66\n","OFF               0.737   0.444     0.554  0.793  0.707       63    38\n","weighted avg      0.592   0.533     0.545  0.737  0.581      180   180\n","\n","Branched-CNN\n","6/6 [==============================] - 1s 6ms/step\n","Accuracy: 0.55\n","MCC: 0.287\n","F1 (Weighted): 0.493\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.526   0.800     0.635  0.710  0.618       75   114\n","FBwM              0.400   0.048     0.085  0.567  0.261       42     5\n","OFF               0.607   0.587     0.597  0.757  0.623       63    61\n","weighted avg      0.525   0.550     0.493  0.705  0.557      180   180\n","\n","Fold 4\n","CNN (T/C Collapse)\n","6/6 [==============================] - 0s 2ms/step\n","Accuracy: 0.65\n","MCC: 0.455\n","F1 (Weighted): 0.642\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.637   0.773     0.699  0.812  0.749       75    91\n","FBwM              0.516   0.381     0.438  0.673  0.471       42    31\n","OFF               0.741   0.683     0.711  0.845  0.723       63    58\n","weighted avg      0.645   0.650     0.642  0.803  0.688      180   180\n","\n","CNN\n","6/6 [==============================] - 0s 2ms/step\n","Accuracy: 0.55\n","MCC: 0.3\n","F1 (Weighted): 0.539\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.595   0.627     0.610  0.751  0.694       75    79\n","FBwM              0.281   0.214     0.243  0.695  0.329       42    32\n","OFF               0.623   0.683     0.652  0.818  0.733       63    69\n","weighted avg      0.532   0.550     0.539  0.760  0.629      180   180\n","\n","CNN-LSTM\n","6/6 [==============================] - 0s 3ms/step\n","Accuracy: 0.55\n","MCC: 0.32\n","F1 (Weighted): 0.561\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.649   0.640     0.644  0.792  0.733       75    74\n","FBwM              0.286   0.381     0.327  0.593  0.271       42    56\n","OFF               0.700   0.556     0.619  0.794  0.693       63    50\n","weighted avg      0.582   0.550     0.561  0.742  0.615      180   180\n","\n","Branched-CNN\n","6/6 [==============================] - 1s 6ms/step\n","Accuracy: 0.456\n","MCC: 0.198\n","F1 (Weighted): 0.47\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.536   0.493     0.514  0.706  0.632       75    69\n","FBwM              0.270   0.476     0.345  0.540  0.259       42    74\n","OFF               0.676   0.397     0.500  0.752  0.623       63    37\n","weighted avg      0.523   0.456     0.470  0.663  0.527      180   180\n","\n","Fold 5\n","CNN (T/C Collapse)\n","6/6 [==============================] - 0s 2ms/step\n","Accuracy: 0.644\n","MCC: 0.453\n","F1 (Weighted): 0.648\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.659   0.720     0.688  0.834  0.790       75    82\n","FBwM              0.386   0.405     0.395  0.745  0.404       42    44\n","OFF               0.833   0.714     0.769  0.875  0.832       63    54\n","weighted avg      0.656   0.644     0.648  0.829  0.731      180   180\n","\n","CNN\n","6/6 [==============================] - 0s 3ms/step\n","Accuracy: 0.628\n","MCC: 0.428\n","F1 (Weighted): 0.627\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.690   0.653     0.671  0.816  0.770       75    71\n","FBwM              0.463   0.452     0.458  0.714  0.389       42    41\n","OFF               0.662   0.714     0.687  0.832  0.741       63    68\n","weighted avg      0.627   0.628     0.627  0.795  0.686      180   180\n","\n","CNN-LSTM\n","6/6 [==============================] - 0s 3ms/step\n","Accuracy: 0.639\n","MCC: 0.454\n","F1 (Weighted): 0.649\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.743   0.693     0.717  0.851  0.783       75    70\n","FBwM              0.370   0.476     0.417  0.676  0.396       42    54\n","OFF               0.768   0.683     0.723  0.888  0.829       63    56\n","weighted avg      0.665   0.639     0.649  0.820  0.706      180   180\n","\n","Branched-CNN\n","6/6 [==============================] - 1s 9ms/step\n","Accuracy: 0.472\n","MCC: 0.184\n","F1 (Weighted): 0.474\n","Classification Report:\n","              precision  recall  f1-score    auc  auprc  support  pred\n","FB                0.477   0.560     0.515  0.633  0.596       75    88\n","FBwM              0.279   0.286     0.282  0.668  0.382       42    43\n","OFF               0.633   0.492     0.554  0.712  0.586       63    49\n","weighted avg      0.485   0.472     0.474  0.679  0.519      180   180\n","\n"]}],"source":["# Define objects to store training history and results\n","cv_models = {\n","    \"cnn\": {\"models\": [], \"histories\": []},\n","    \"cnn-lstm\": {\"models\": [], \"histories\": []},\n","    \"branched-cnn\": {\"models\": [], \"histories\": []},\n","    \"cnn-xtra\": {\"models\": [], \"histories\": []}\n","}\n","\n","cv_test_results = {\n","    \"cnn\": {\"accuracy\": [], \"MCC\": [], \"F1\": [], \"classification_report\": [], \"confusion_matrix\": []},\n","    \"cnn-lstm\": {\"accuracy\": [], \"MCC\": [], \"F1\": [], \"classification_report\": [], \"confusion_matrix\": []},\n","    \"branched-cnn\": {\"accuracy\": [], \"MCC\": [], \"F1\": [], \"classification_report\": [], \"confusion_matrix\": []},\n","    \"cnn-xtra\": {\"accuracy\": [], \"MCC\": [], \"F1\": [], \"classification_report\": [], \"confusion_matrix\": []}\n","}\n","\n","cv_predictions = {\n","    \"cnn\": {\"y_true\": [], \"y_score\": {}, \"y_pred\": {}},\n","    \"cnn-lstm\": {\"y_true\": [], \"y_score\": {}, \"y_pred\": {}},\n","    \"branched-cnn\": {\"y_true\": [], \"y_score\": {}, \"y_pred\": {}},\n","    \"cnn-xtra\": {\"y_true\": [], \"y_score\": {}, \"y_pred\": {}}\n","}\n","\n","cv_predictions[\"cnn\"][\"y_true\"] = y_test\n","cv_predictions[\"cnn-lstm\"][\"y_true\"] = y_test\n","cv_predictions[\"branched-cnn\"][\"y_true\"] = y_test\n","cv_predictions[\"cnn-xtra\"][\"y_true\"] = y_test\n","\n","# Define the K-fold parameters\n","kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n","\n","# Train, validate, and evaluate the model for each fold\n","for fold, (train_idx, val_idx) in enumerate(kfold.split(x_train, y_train)):\n","    \n","  print(f\"Fold {fold+1}\")\n","  \n","  # Get the training and validation data for this fold\n","  x_train_fold, y_train_fold = x_train[train_idx], y_train_dummy[train_idx]\n","  x_val_fold, y_val_fold = x_train[val_idx], y_train_dummy[val_idx]\n","\n","  # Scale data\n","  scaler = StandardScaler()\n","  x_train_fold = scaler.fit_transform(x_train_fold.reshape(-1, x_train_fold.shape[-1])).reshape(x_train_fold.shape)\n","  x_val_fold = scaler.transform(x_val_fold.reshape(-1, x_val_fold.shape[-1])).reshape(x_val_fold.shape)\n","  x_test_fold = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n","\n","  # Prepare for keras usage\n","  x_train_fold = x_train_fold.reshape(x_train_fold.shape[0], x_train_fold.shape[1], x_train_fold.shape[2], x_train_fold.shape[3], 1)\n","  x_val_fold = x_val_fold.reshape(x_val_fold.shape[0], x_val_fold.shape[1], x_val_fold.shape[2], x_val_fold.shape[3], 1)\n","  x_test_fold = x_test_fold.reshape(x_test_fold.shape[0], x_test_fold.shape[1], x_test_fold.shape[2], x_test_fold.shape[3], 1)\n","\n","  # SHAPE: (n_batch, 15, 10, 2, 1)\n","\n","  # CNN (Time/Coord Collapsed) -------------------------------------------------\n","  # Reshape to (n_batch, 10, 30, 1)\n","  # First: (n_batch, 10, 15, 2, 1)\n","  x_train_fold_cnnx = np.transpose(x_train_fold, (0, 2, 1, 3, 4))\n","  x_val_fold_cnnx = np.transpose(x_val_fold, (0, 2, 1, 3, 4))\n","  x_test_fold_cnnx = np.transpose(x_test_fold, (0, 2, 1, 3, 4))\n","\n","  # Second: (n_batch, 10, 30, 1)\n","  x_train_fold_cnnx = x_train_fold_cnnx.reshape(x_train_fold_cnnx.shape[0], 10, 30, 1)\n","  x_val_fold_cnnx = x_val_fold_cnnx.reshape(x_val_fold_cnnx.shape[0], 10, 30, 1)\n","  x_test_fold_cnnx = x_test_fold_cnnx.reshape(x_test_fold_cnnx.shape[0], 10, 30, 1)\n","\n","  # Set model\n","  cnnx_model = CNN_tc_collapse(\n","    x_shape=(10,30,1), \n","    dropout=dropout, \n","    l1=l1, l2=l2,\n","    learning_rate=1e-4,\n","    n_targets=3)\n","\n","  # Train the model on the training data for this fold\n","  cnnx_history = cnnx_model.fit(\n","    x_train_fold_cnnx, y_train_fold, \n","    epochs=epochs, batch_size=batch_size, \n","    validation_data=(x_val_fold_cnnx, y_val_fold),\n","    class_weight=classWeights,\n","    callbacks=[early_stopping, reduce_lr_plateau],\n","    verbose=0)\n","  \n","  # Store model and history\n","  cv_models[\"cnn-xtra\"][\"models\"].append(cnnx_model)\n","  cv_models[\"cnn-xtra\"][\"histories\"].append(cnnx_history)\n","\n","  # Evaluate predictions\n","  print(\"CNN (T/C Collapse)\")\n","  y_test_score = cnnx_model.predict(x=x_test_fold_cnnx) # Make predictions\n","  y_test_pred = np.argmax(y_test_score, axis=1)         # Get multiclass predictions from scores\n","  test_acc, test_mcc, test_f1, classification_report, confusion_mat = evaluate_predictions(\n","      y_true=y_test, y_test_score=y_test_score, y_test_pred=y_test_pred, target_names_dict=target_names_dict)\n","  \n","  # Store predictions\n","  cv_predictions[\"cnn-xtra\"][\"y_score\"][f\"{fold+1}\"] = y_test_score\n","  cv_predictions[\"cnn-xtra\"][\"y_pred\"][f\"{fold+1}\"] = y_test_pred\n","\n","  # Store metrics\n","  cv_test_results[\"cnn-xtra\"][\"accuracy\"].append(test_acc)\n","  cv_test_results[\"cnn-xtra\"][\"MCC\"].append(test_mcc)\n","  cv_test_results[\"cnn-xtra\"][\"F1\"].append(test_f1)\n","  cv_test_results[\"cnn-xtra\"][\"classification_report\"].append(classification_report)\n","  cv_test_results[\"cnn-xtra\"][\"confusion_matrix\"].append(confusion_mat)\n","\n","  # Clear keras session\n","  K.clear_session()\n","\n","\n","  # CNN ------------------------------------------------------------------------\n","  # Set model\n","  cnn_model = CNN(\n","    x_shape=(15,10,2,1), \n","    kernel_size=(3,3,1), \n","    dropout=dropout, \n","    l1=l1, l2=l2, \n","    learning_rate=1e-4,\n","    n_targets=3)\n","\n","  # Train the model on the training data for this fold\n","  cnn_history = cnn_model.fit(\n","    x_train_fold, y_train_fold, \n","    epochs=epochs, batch_size=batch_size, \n","    validation_data=(x_val_fold, y_val_fold),\n","    class_weight=classWeights,\n","    callbacks=[early_stopping, reduce_lr_plateau],\n","    verbose=0)\n","  \n","  # Store model and history\n","  cv_models[\"cnn\"][\"models\"].append(cnn_model)\n","  cv_models[\"cnn\"][\"histories\"].append(cnn_history)\n","\n","  # Evaluate predictions\n","  print(\"CNN\")\n","  y_test_score = cnn_model.predict(x=x_test_fold) # Make predictions\n","  y_test_pred = np.argmax(y_test_score, axis=1)   # Get multiclass predictions from scores\n","  test_acc, test_mcc, test_f1, classification_report, confusion_mat = evaluate_predictions(\n","      y_true=y_test, y_test_score=y_test_score, y_test_pred=y_test_pred, target_names_dict=target_names_dict)\n","  \n","  # Store predictions\n","  cv_predictions[\"cnn\"][\"y_score\"][f\"{fold+1}\"] = y_test_score\n","  cv_predictions[\"cnn\"][\"y_pred\"][f\"{fold+1}\"] = y_test_pred\n","\n","  # Store metrics\n","  cv_test_results[\"cnn\"][\"accuracy\"].append(test_acc)\n","  cv_test_results[\"cnn\"][\"MCC\"].append(test_mcc)\n","  cv_test_results[\"cnn\"][\"F1\"].append(test_f1)\n","  cv_test_results[\"cnn\"][\"classification_report\"].append(classification_report)\n","  cv_test_results[\"cnn\"][\"confusion_matrix\"].append(confusion_mat)\n","\n","  # Clear keras session\n","  K.clear_session()\n","\n","\n","  # CNN-LSTM -------------------------------------------------------------------\n","  # Set model\n","  cnn_lstm_model = CNN_LSTM(\n","    x_shape=(15,10,2,1), \n","    kernel_size=(3,3,1),\n","    dropout=dropout, \n","    l1=l1, l2=l2, \n","    learning_rate=1e-4,\n","    n_targets=3)\n","\n","  # Train the model on the training data for this fold\n","  cnn_lstm_history = cnn_lstm_model.fit(\n","    x_train_fold, y_train_fold, \n","    epochs=epochs, batch_size=batch_size, \n","    validation_data=(x_val_fold, y_val_fold),\n","    class_weight=classWeights,\n","    callbacks=[early_stopping, reduce_lr_plateau],\n","    verbose=0)\n","  \n","  # Store model and history\n","  cv_models[\"cnn-lstm\"][\"models\"].append(cnn_lstm_model)\n","  cv_models[\"cnn-lstm\"][\"histories\"].append(cnn_lstm_history)\n","\n","  # Evaluate predictions\n","  print(\"CNN-LSTM\")\n","  y_test_score = cnn_lstm_model.predict(x=x_test_fold) # Make predictions\n","  y_test_pred = np.argmax(y_test_score, axis=1)        # Get multiclass predictions from scores\n","  test_acc, test_mcc, test_f1, classification_report, confusion_mat = evaluate_predictions(\n","      y_true=y_test, y_test_score=y_test_score, y_test_pred=y_test_pred, target_names_dict=target_names_dict)\n","  \n","  # Store predictions\n","  cv_predictions[\"cnn-lstm\"][\"y_score\"][f\"{fold+1}\"] = y_test_score\n","  cv_predictions[\"cnn-lstm\"][\"y_pred\"][f\"{fold+1}\"] = y_test_pred\n","\n","  # Store metrics\n","  cv_test_results[\"cnn-lstm\"][\"accuracy\"].append(test_acc)\n","  cv_test_results[\"cnn-lstm\"][\"MCC\"].append(test_mcc)\n","  cv_test_results[\"cnn-lstm\"][\"F1\"].append(test_f1)\n","  cv_test_results[\"cnn-lstm\"][\"classification_report\"].append(classification_report)\n","  cv_test_results[\"cnn-lstm\"][\"confusion_matrix\"].append(confusion_mat)\n","\n","  # Clear keras session\n","  K.clear_session()\n","\n","\n","  # Branched model data prep ---------------------------------------------------\n","  # Reshape data to (n, 10, 15, 2, 1)\n","  x_train_fold_branched = np.transpose(x_train_fold, (0, 2, 1, 3, 4))\n","  x_val_fold_branched = np.transpose(x_val_fold, (0, 2, 1, 3, 4))\n","  x_test_fold_branched = np.transpose(x_test_fold, (0, 2, 1, 3, 4))\n","\n","  # Reshape data to 10 * (n, 15, 2, 1)\n","  x_train_fold_branched = split_x_by_keypoint(x_train_fold_branched)\n","  x_val_fold_branched = split_x_by_keypoint(x_val_fold_branched)\n","  x_test_fold_branched = split_x_by_keypoint(x_test_fold_branched)\n","\n","\n","  # Branched-CNN ---------------------------------------------------------------\n","  # Set model\n","  branched_cnn_model = CNN_branched(\n","    x_shape=(15,2,1), \n","    dropout=dropout, \n","    l1=l1, l2=l2, \n","    learning_rate=1e-4,\n","    n_targets=3)\n","\n","  # Train the model on the training data for this fold\n","  branched_cnn_history = branched_cnn_model.fit(\n","    x_train_fold_branched, y_train_fold, \n","    epochs=epochs, batch_size=batch_size, \n","    validation_data=(x_val_fold_branched, y_val_fold),\n","    class_weight=classWeights,\n","    callbacks=[early_stopping, reduce_lr_plateau],\n","    verbose=0)\n","  \n","  # Store model and history\n","  cv_models[\"branched-cnn\"][\"models\"].append(branched_cnn_model)\n","  cv_models[\"branched-cnn\"][\"histories\"].append(branched_cnn_history)\n","\n","  # Evaluate predictions\n","  print(\"Branched-CNN\")\n","  y_test_score = branched_cnn_model.predict(x=x_test_fold_branched) # Make predictions\n","  y_test_pred = np.argmax(y_test_score, axis=1)                     # Get multiclass predictions from scores\n","  test_acc, test_mcc, test_f1, classification_report, confusion_mat = evaluate_predictions(\n","      y_true=y_test, y_test_score=y_test_score, y_test_pred=y_test_pred, target_names_dict=target_names_dict)\n","  \n","  # Store predictions\n","  cv_predictions[\"branched-cnn\"][\"y_score\"][f\"{fold+1}\"] = y_test_score\n","  cv_predictions[\"branched-cnn\"][\"y_pred\"][f\"{fold+1}\"] = y_test_pred\n","\n","  # Store metrics\n","  cv_test_results[\"branched-cnn\"][\"accuracy\"].append(test_acc)\n","  cv_test_results[\"branched-cnn\"][\"MCC\"].append(test_mcc)\n","  cv_test_results[\"branched-cnn\"][\"F1\"].append(test_f1)\n","  cv_test_results[\"branched-cnn\"][\"classification_report\"].append(classification_report)\n","  cv_test_results[\"branched-cnn\"][\"confusion_matrix\"].append(confusion_mat)\n","\n","  # Clear keras session\n","  K.clear_session()\n"]},{"cell_type":"code","source":["history_file = os.path.join(root_path, \"modeling/saved-modeling-results/kfold-model-history.buehler.pkl\")\n","with open(history_file, 'wb') as pkl_file:\n","  pickle.dump(cv_models, pkl_file)"],"metadata":{"id":"uAS34jIYZEsc","executionInfo":{"status":"ok","timestamp":1682643930319,"user_tz":360,"elapsed":6945,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["results_file = os.path.join(root_path, \"modeling/saved-modeling-results/kfold-model-results.buehler.pkl\")\n","with open(results_file, 'wb') as pkl_file:\n","  pickle.dump(cv_test_results, pkl_file)"],"metadata":{"id":"pm7XT7qnZL08","executionInfo":{"status":"ok","timestamp":1682643945336,"user_tz":360,"elapsed":309,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["predictions_file = os.path.join(root_path, \"modeling/saved-modeling-results/kfold-model-predictions.buehler.pkl\")\n","with open(predictions_file, 'wb') as pkl_file:\n","  pickle.dump(cv_predictions, pkl_file)"],"metadata":{"id":"Qrw-cgdzZQ6v","executionInfo":{"status":"ok","timestamp":1682643972215,"user_tz":360,"elapsed":326,"user":{"displayName":"Brenton Graham","userId":"13609192727467696213"}}},"execution_count":40,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1EYaWYbBg5-xqG7Otlq972ASVXlU90SkT","timestamp":1682559885459},{"file_id":"1Fdc7Tm11RxgVL-TM7DZEXavp4i-DQrER","timestamp":1682390522713}],"authorship_tag":"ABX9TyOlvTkgyQfCUv1iGbJJJrSU"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}